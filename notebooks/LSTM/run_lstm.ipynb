{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from skimage.transform import resize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import wandb\n",
    "import magnet\n",
    "\n",
    "# import analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.lstm = nn.LSTM(1, 64, num_layers=1, batch_first=True, bidirectional=False)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :] # Get last output only (many-to-one)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "YAML_CONFIG = OmegaConf.load(\"lstm.yaml\")\n",
    "CLI_CONFIG = OmegaConf.from_cli()\n",
    "CONFIG = OmegaConf.merge(YAML_CONFIG, CLI_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "random.seed(CONFIG.SEED)\n",
    "np.random.seed(CONFIG.SEED)\n",
    "torch.manual_seed(CONFIG.SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Setup CPU or GPU\n",
    "if CONFIG.USE_GPU and not torch.cuda.is_available():\n",
    "    raise ValueError(\"GPU not detected but CONFIG.USE_GPU is set to True.\")\n",
    "device = torch.device(\"cuda\" if CONFIG.USE_GPU else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup dataset and dataloader\n",
    "# NOTE(seungjaeryanlee): Load saved dataset for speed\n",
    "dataset = magnet.PyTorchVoltageToCoreLossDataset(download_path=\"../../data/\", download=True)\n",
    "train_size = int(0.6 * len(dataset))\n",
    "valid_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if CONFIG.USE_GPU else {}\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True, **kwargs)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup neural network and optimizer\n",
    "net = Net().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=CONFIG.LR)\n",
    "# Log number of parameters\n",
    "CONFIG.NUM_PARAMETERS = count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup wandb\n",
    "# wandb.init(project=\"MagNet\", config=OmegaConf.to_container(CONFIG), reinit=True)\n",
    "# wandb.watch(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 Train 0.00148 Valid 0.00130\n",
      "Epoch  2 Train 0.00129 Valid 0.00114\n",
      "Epoch  3 Train 0.00111 Valid 0.00100\n",
      "Epoch  4 Train 0.00098 Valid 0.00087\n",
      "Epoch  5 Train 0.00084 Valid 0.00076\n",
      "Epoch  6 Train 0.00074 Valid 0.00067\n",
      "Epoch  7 Train 0.00064 Valid 0.00058\n",
      "Epoch  8 Train 0.00056 Valid 0.00051\n",
      "Epoch  9 Train 0.00059 Valid 0.00044\n",
      "Epoch 10 Train 0.00043 Valid 0.00038\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch_i in range(1, CONFIG.NUM_EPOCH+1):\n",
    "    # Train for one epoch\n",
    "    epoch_train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.unsqueeze(2) # LSTM requires 3 dimension: (batch size, # timesteps, # features)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs.to(device))\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    # Compute Validation Loss\n",
    "    with torch.no_grad():\n",
    "        epoch_valid_loss = 0\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs = inputs.unsqueeze(2) # LSTM requires 3 dimension: (batch size, # timesteps, # features)\n",
    "            outputs = net(inputs.to(device))\n",
    "            loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "            epoch_valid_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch_i:2d} \"\n",
    "        f\"Train {epoch_train_loss / len(train_dataset):.5f} \"\n",
    "        f\"Valid {epoch_valid_loss / len(valid_dataset):.5f}\")\n",
    "#     wandb.log({\n",
    "#         \"train/loss\": epoch_train_loss / len(train_dataset),\n",
    "#         \"valid/loss\": epoch_valid_loss / len(valid_dataset),\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.00004100\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "net.eval()\n",
    "y_meas = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.unsqueeze(2) # LSTM requires 3 dimension: (batch size, # timesteps, # features)\n",
    "        y_pred.append(net(inputs.to(device)))\n",
    "        y_meas.append(labels.to(device))\n",
    "\n",
    "y_meas = torch.cat(y_meas, dim=0)\n",
    "y_pred = torch.cat(y_pred, dim=0)\n",
    "test_mse_loss = F.mse_loss(y_meas, y_pred).item() / len(test_dataset)\n",
    "print(f\"Test Loss: {test_mse_loss:.8f}\")\n",
    "# wandb.log({\"test/loss\": test_mse_loss})\n",
    "\n",
    "# Analysis\n",
    "# wandb.log({\n",
    "#     \"test/prediction_vs_target\": wandb.Image(analysis.get_scatter_plot(y_pred, y_meas)),\n",
    "#     \"test/prediction_vs_target_histogram\": wandb.Image(analysis.get_two_histograms(y_pred, y_meas)),\n",
    "#     \"test/error_histogram\": wandb.Image(analysis.get_error_histogram(y_pred, y_meas)),\n",
    "# })\n",
    "\n",
    "# Close wandb\n",
    "# wandb.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37_pytorch]",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
