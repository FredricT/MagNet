{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"v2p_LSTM-FC_tuneHyper.ipynb","provenance":[],"mount_file_id":"1g5SQkPpagFhtl7engHkZb0KLTPsreBFU","authorship_tag":"ABX9TyMCfXsA9b5od69g7C1eoZ8j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"zyNohZ80U1wy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1596060940509,"user_tz":240,"elapsed":22536,"user":{"displayName":"Haoran Li","photoUrl":"","userId":"12345382914336844077"}},"outputId":"0964c8af-5ce5-4603-c85b-3b017643e139"},"source":["#import os\n","#assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n","#VERSION = \"nightly\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n","#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n","#!python pytorch-xla-env-setup.py --version $VERSION\n","#import torch_xla\n","#import torch_xla.core.xla_model as xm\n","\n","!pip install omegaconf\n","!pip install optuna\n","\n","import random\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from omegaconf import OmegaConf\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","# import wandb\n","import optuna\n","\n","\n","\n","\n","\n","class Net(nn.Module):\n","    def __init__(self,trial):\n","        super(Net, self).__init__()\n","        self.lstm = nn.LSTM(1, 42, num_layers=1, batch_first=True, bidirectional=False)\n","        \n","        #n_units_FC2 = trial.suggest_int(\"n_units_FC2\", 4, 64)\n","        #n_units_FC3 = trial.suggest_int(\"n_units_FC3\", 4, 64)\n","        #n_units_FC4 = trial.suggest_int(\"n_units_FC4\", 4, 64)\n","\n","        n_units_FC2 = 25\n","        n_units_FC3 = 25\n","        n_units_FC4 = 25\n","\n","        self.fc_layers = nn.Sequential(\n","            nn.Linear(42, n_units_FC2),\n","            nn.ReLU(),\n","            nn.Linear(n_units_FC2, 2),\n","            nn.ReLU(),\n","            nn.Linear(2, n_units_FC3),\n","            nn.ReLU(),\n","            nn.Linear(n_units_FC3, n_units_FC4),\n","            nn.ReLU(),\n","            nn.Linear(n_units_FC4, 1)\n","        )\n","\n","    def forward(self, x):\n","        x, _ = self.lstm(x)\n","        x = x[:, -1, :] # Get last output only (many-to-one)\n","        x = self.fc_layers(x)\n","        return x\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def get_dataset(data_length=400, sample_rate=2e-6):\n","    # Load CSV Files\n","    # TODO: Use numpy instead of pandas\n","    \n","    V = pd.read_csv('drive/My Drive/Colab Notebooks/Data/Dummy_volt.csv', header=None).iloc[:,0:data_length]\n","    # B = pd.read_csv('D:\\Dropbox (Princeton)\\MagNet_TestData_dummy\\Dummy_flux.csv', header=None)\n","    # F = pd.read_csv('D:\\Dropbox (Princeton)\\MagNet_TestData_dummy\\Dummy_freq.csv', header=None)\n","    P = pd.read_csv('drive/My Drive/Colab Notebooks/Data/Dummy_loss.csv', header=None)\n","\n","    # F = F.apply(np.log10)\n","    # B = B.apply(np.log10)\n","    P = P.apply(np.log10)\n","\n","    I = pd.concat([P], axis=1, ignore_index=True)\n","\n","    # Reshape data\n","    in_tensors = torch.from_numpy(V.to_numpy()).view(-1, data_length, 1)\n","    out_tensors = torch.from_numpy(I.to_numpy()).view(-1, 1)\n","\n","    # Save data as CSV\n","    np.save(\"dataset.lstm.in.npy\", in_tensors.numpy())\n","    np.save(\"dataset.lstm.out.npy\", out_tensors.numpy())\n","\n","    return torch.utils.data.TensorDataset(in_tensors, out_tensors)\n","\n","\n","def load_dataset(in_filename=\"dataset.lstm.in.npy\", out_filename=\"dataset.lstm.out.npy\"):\n","    in_tensors = torch.from_numpy(np.load(in_filename))\n","    out_tensors = torch.from_numpy(np.load(out_filename))\n","\n","    return torch.utils.data.TensorDataset(in_tensors, out_tensors)\n","\n","\n","def objective(trial):\n","    # Load Configuration\n","    YAML_CONFIG = OmegaConf.load(\"drive/My Drive/Colab Notebooks/lstm.yaml\")\n","    CLI_CONFIG = OmegaConf.from_cli()\n","    DEFAULT_CONFIG = OmegaConf.merge(YAML_CONFIG, CLI_CONFIG)\n","\n","    # Get hyperparameters from Optuna\n","    OPTUNA_CONFIG = OmegaConf.create({\n","        \"LR\": trial.suggest_loguniform(\"LR\", 1e-5, 1e-2),\n","    })\n","    CONFIG = OmegaConf.merge(DEFAULT_CONFIG, OPTUNA_CONFIG)\n","\n","    # Reproducibility\n","    random.seed(CONFIG.SEED)\n","    np.random.seed(CONFIG.SEED)\n","    torch.manual_seed(CONFIG.SEED)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    # Setup CPU or GPU\n","    if CONFIG.USE_GPU and not torch.cuda.is_available():\n","        raise ValueError(\"GPU not detected but CONFIG.USE_GPU is set to True.\")\n","    device = torch.device(\"cuda\")\n","    #device = torch.device(\"cpu\")\n","    #device = xm.xla_device()\n","\n","    # Setup dataset and dataloader\n","    # NOTE(seungjaeryanlee): Load saved dataset for speed\n","    dataset = get_dataset()\n","    #dataset = load_dataset()\n","    train_size = int(0.6 * len(dataset))\n","    valid_size = int(0.2 * len(dataset))\n","    test_size = len(dataset) - train_size - valid_size\n","    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n","    kwargs = {'num_workers': 1, 'pin_memory': True} if CONFIG.USE_GPU else {}\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True, **kwargs)\n","    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False, **kwargs)\n","    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False, **kwargs)\n","\n","    # Setup neural network and optimizer\n","    net = Net(trial).double().to(device)\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(net.parameters(), lr=CONFIG.LR)\n","    # Log number of parameters\n","    CONFIG.NUM_PARAMETERS = count_parameters(net)\n","\n","    # Setup wandb\n","    # wandb.init(project=\"MagNet\", config=CONFIG)\n","    # wandb.watch(net)\n","\n","    # Training\n","    for epoch_i in range(1, CONFIG.NUM_EPOCH+1):\n","        # Train for one epoch\n","        epoch_train_loss = 0\n","        for inputs, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = net(inputs.to(device))\n","            loss = criterion(outputs, labels.to(device))\n","            loss.backward()\n","            optimizer.step()\n","\n","            epoch_train_loss += loss.item()\n","\n","        # Compute Validation Loss\n","        with torch.no_grad():\n","            epoch_valid_loss = 0\n","            for inputs, labels in valid_loader:\n","                outputs = net(inputs.to(device))\n","                loss = criterion(outputs, labels.to(device))\n","\n","                epoch_valid_loss += loss.item()\n","\n","        print(f\"Epoch {epoch_i:2d} \"\n","            f\"Train {epoch_train_loss / len(train_dataset):.5f} \"\n","            f\"Valid {epoch_valid_loss / len(valid_dataset):.5f}\")\n","        # wandb.log({\n","        #     \"train/loss\": epoch_train_loss / len(train_dataset),\n","        #     \"valid/loss\": epoch_valid_loss / len(valid_dataset),\n","        # })\n","\n","    # Evaluation\n","    net.eval()\n","    y_meas = []\n","    y_pred = []\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            y_pred.append(net(inputs.to(device)))\n","            y_meas.append(labels.to(device))\n","\n","    y_meas = torch.cat(y_meas, dim=0)\n","    y_pred = torch.cat(y_pred, dim=0)\n","    # print(f\"Test Loss: {F.mse_loss(y_meas, y_pred).item() / len(test_dataset):.8f}\")\n","    # wandb.log({\"test/loss\": F.mse_loss(y_meas, y_pred).item() / len(test_dataset)})\n","\n","    # Predicton vs Target Plot\n","    fig, ax = plt.subplots(1, 1)\n","    fig.set_size_inches(8, 8)\n","    ax.scatter(y_meas.cpu().numpy(), y_pred.cpu().numpy(), label=\"Prediction\")\n","    ax.plot(y_meas.cpu().numpy(), y_meas.cpu().numpy(), 'k--', label=\"Target\")\n","    ax.grid(True)\n","    ax.legend()\n","    # wandb.log({\"prediction_vs_target\": wandb.Image(fig)})\n","    \n","    y_pred = y_pred.cpu().numpy()\n","    y_pred = 10**y_pred\n","    y_meas = y_meas.cpu().numpy()\n","    y_meas = 10**y_meas\n","\n","    # Relative Error\n","    Error_re1 = abs(y_pred-y_meas)/abs(y_meas)*100\n","    Error_re1[np.where(Error_re1>500)] = 500\n","    # Error_re_max1 = np.max(Error_re1);\n","    Error_re_avg1 = np.mean(Error_re1);\n","    # print(f\"Relative Error: {Error_re_avg1:.8f}\")\n","    # wandb.log({\"Relative Error\": Error_re_avg1})\n","    \n","    # np.savetxt(\"pred.csv\", y_pred.cpu().numpy())\n","    # np.savetxt(\"meas.csv\", y_meas.cpu().numpy())\n","    return Error_re_avg1\n","\n","\n","if __name__ == \"__main__\":\n","    study = optuna.create_study(\n","        study_name = 'LSTM-FC_linear',\n","        direction=\"minimize\",\n","        storage = 'sqlite:///example.db',\n","        load_if_exists=True)\n","    study.optimize(objective, n_trials=10)\n","\n","    pruned_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED]\n","    complete_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE]\n","\n","    print(\"Study statistics: \")\n","    print(\"  Number of finished trials: \", len(study.trials))\n","    print(\"  Number of pruned trials: \", len(pruned_trials))\n","    print(\"  Number of complete trials: \", len(complete_trials))\n","\n","    print(\"Best trial:\")\n","    trial = study.best_trial\n","\n","    print(\"  Value: \", trial.value)\n","\n","    print(\"  Params: \")\n","    for key, value in trial.params.items():\n","        print(\"    {}: {}\".format(key, value))\n","\n","    optuna.visualization.plot_optimization_history(study)\n","    #optuna.visualization.plot_contour(study, params=['n_units_FC2', 'n_units_FC3'])\n","    #optuna.visualization.plot_contour(study, params=['n_units_FC2', 'n_units_FC4'])\n","    #optuna.visualization.plot_contour(study, params=['n_units_FC3', 'n_units_FC4'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: omegaconf in /usr/local/lib/python3.6/dist-packages (2.0.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from omegaconf) (3.13)\n","Requirement already satisfied: dataclasses; python_version == \"3.6\" in /usr/local/lib/python3.6/dist-packages (from omegaconf) (0.7)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from omegaconf) (3.7.4.2)\n","Requirement already satisfied: optuna in /usr/local/lib/python3.6/dist-packages (2.0.0)\n","Requirement already satisfied: alembic in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.2)\n","Requirement already satisfied: cliff in /usr/local/lib/python3.6/dist-packages (from optuna) (3.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.18.5)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.18)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from optuna) (20.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.16.0)\n","Requirement already satisfied: cmaes>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from optuna) (0.6.0)\n","Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.6/dist-packages (from optuna) (4.2.1)\n","Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.0.4)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (2.8.1)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.1.3)\n","Requirement already satisfied: cmd2!=0.8.3,>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.2.1)\n","Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (0.7.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.15.0)\n","Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.13)\n","Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (5.4.5)\n","Requirement already satisfied: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (2.4.7)\n","Requirement already satisfied: stevedore>=1.20.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.2.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n","Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (19.3.0)\n","Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.4.3)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n","Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (1.8.0)\n","Requirement already satisfied: importlib-metadata>=1.6.0; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (1.7.0)\n","Requirement already satisfied: setuptools>=34.4 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (49.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2!=0.8.3,>=0.8.0->cliff->optuna) (3.1.0)\n"],"name":"stdout"},{"output_type":"stream","text":["[I 2020-07-29 22:15:25,320] Using an existing study with name 'LSTM-FC_linear' instead of creating a new one.\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch  1 Train 0.03645 Valid 0.00832\n","Epoch  2 Train 0.00374 Valid 0.00206\n","Epoch  3 Train 0.00134 Valid 0.00089\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-9ab4c8f34f53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sqlite:///example.db'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         load_if_exists=True)\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0mpruned_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRUNED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 self._optimize_sequential(\n\u001b[0;32m--> 292\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m                 )\n\u001b[1;32m    294\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial, time_start)\u001b[0m\n\u001b[1;32m    652\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_progress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Trial {} pruned. {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-9ab4c8f34f53>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# Compute Validation Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}